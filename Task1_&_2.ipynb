{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxJULs3J3F34Yh7e8Y3Cs9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinutha18-m/Yardstick/blob/main/Task1_%26_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_J1i0rQTAvg",
        "outputId": "5a862be7-e530-4cca-d20a-5e97062a66ad"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (0.31.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from groq import Groq\n",
        "\n",
        "client = Groq(\n",
        "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
        ")\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LUvPaHUS7HH",
        "outputId": "03b83da1-2b21-41fb-f287-974debd3aac3"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fast language models are crucial in today's technology-driven world, and their importance can be seen in several areas. Here are some reasons why fast language models are essential:\n",
            "\n",
            "1. **Efficient Processing**: Fast language models can process vast amounts of text data quickly, which is vital for applications that require real-time or near-real-time processing, such as chatbots, virtual assistants, and language translation software.\n",
            "2. **Improved User Experience**: Quick response times are essential for a seamless user experience. Fast language models enable applications to respond promptly to user queries, reducing wait times and improving overall satisfaction.\n",
            "3. **Scalability**: As the amount of text data grows, fast language models can handle the increased volume without compromising performance. This scalability is critical for large-scale applications, such as language translation platforms, text summarization tools, and sentiment analysis software.\n",
            "4. **Real-time Applications**: Fast language models are crucial for real-time applications, such as:\n",
            "\t* Live language translation\n",
            "\t* Real-time sentiment analysis\n",
            "\t* Live chat support\n",
            "\t* Speech recognition systems\n",
            "5. **Competitive Advantage**: Organizations that adopt fast language models can gain a competitive advantage by:\n",
            "\t* Responding quickly to customer inquiries\n",
            "\t* Analyzing large amounts of text data efficiently\n",
            "\t* Developing intelligent chatbots and virtual assistants\n",
            "\t* Improving language translation accuracy and speed\n",
            "6. **Cost Savings**: Fast language models can help reduce costs by:\n",
            "\t* Minimizing the need for human intervention\n",
            "\t* Automating tasks, such as data annotation and labeling\n",
            "\t* Reducing the computational resources required for language processing\n",
            "7. **Enhanced Accuracy**: Fast language models can also lead to improved accuracy, as they can process and analyze large amounts of data quickly, reducing the likelihood of errors and inconsistencies.\n",
            "8. **Research and Development**: Fast language models accelerate research and development in areas like:\n",
            "\t* Natural Language Processing (NLP)\n",
            "\t* Machine Learning (ML)\n",
            "\t* Artificial Intelligence (AI)\n",
            "\t* Human-Computer Interaction (HCI)\n",
            "\n",
            "To achieve fast language models, researchers and developers employ various techniques, such as:\n",
            "\n",
            "1. **Model pruning**: Reducing the size of language models while maintaining their performance.\n",
            "2. **Knowledge distillation**: Transferring knowledge from large models to smaller, faster models.\n",
            "3. **Quantization**: Representing model weights and activations using lower-precision data types.\n",
            "4. **Parallel processing**: Distributing computationally intensive tasks across multiple processing units.\n",
            "5. **Specialized hardware**: Utilizing hardware accelerators, such as Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs), designed specifically for AI and ML workloads.\n",
            "\n",
            "In summary, fast language models are essential for efficient processing, improved user experience, scalability, real-time applications, competitive advantage, cost savings, enhanced accuracy, and accelerating research and development in NLP, ML, AI, and HCI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")\n"
      ],
      "metadata": {
        "id": "m9RzGDBHUBLp"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from typing import List, Dict\n",
        "import requests"
      ],
      "metadata": {
        "id": "soeCXjfSPFBW"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textwrap import shorten\n",
        "\n",
        "class ConversationManager:\n",
        "    def __init__(self, summary_interval=3, char_limit=300, word_limit=50):\n",
        "        self.history = []\n",
        "        self.summary_interval = summary_interval\n",
        "        self.char_limit = char_limit\n",
        "        self.word_limit = word_limit\n",
        "        self.assistant_turn_count = 0\n",
        "\n",
        "    def add_message(self, role, content):\n",
        "        self.history.append({\"role\": role, \"content\": content})\n",
        "\n",
        "        if role == \"assistant\":\n",
        "            self.assistant_turn_count += 1\n",
        "\n",
        "        if self.assistant_turn_count > 0 and self.assistant_turn_count % self.summary_interval == 0:\n",
        "            self.summarize_history()\n",
        "\n",
        "    def summarize_history(self):\n",
        "        # Keep all messages, including system summary\n",
        "        non_system_entries = [entry for entry in self.history if entry['role'] != 'system']\n",
        "\n",
        "        if len(non_system_entries) >= 2:\n",
        "            old_messages = non_system_entries[:-1]  # all except last one\n",
        "            last_message = non_system_entries[-1:]  # last message\n",
        "\n",
        "            # Check for existing system summary\n",
        "            system_summary_index = -1\n",
        "            for i, entry in enumerate(self.history):\n",
        "                if entry['role'] == 'system':\n",
        "                    system_summary_index = i\n",
        "                    break\n",
        "\n",
        "            if system_summary_index != -1:\n",
        "                # Update existing system summary\n",
        "                old_messages_text = \" \".join([self.history[system_summary_index]['content']] + [f\"{e['role'].upper()}: {e['content']}\" for e in old_messages])\n",
        "                summary_content = shorten(old_messages_text, width=self.char_limit, placeholder=\"...\")\n",
        "                self.history[system_summary_index]['content'] = f\"{summary_content}\"\n",
        "                self.history = [self.history[system_summary_index]] + last_message\n",
        "            else:\n",
        "                # Add new system summary\n",
        "                old_messages_text = \" \".join(f\"{e['role'].upper()}: {e['content']}\" for e in old_messages)\n",
        "                summary_content = shorten(old_messages_text, width=self.char_limit, placeholder=\"...\")\n",
        "                self.history = [{\"role\": \"system\", \"content\": f\"{summary_content}\"}] + last_message\n",
        "\n",
        "\n",
        "\n",
        "    def truncate_last_n(self, n):\n",
        "        self.history = self.history[-n:]\n",
        "\n",
        "    def truncate_by_char_limit(self, limit):\n",
        "        total_chars = 0\n",
        "        truncated = []\n",
        "        for entry in reversed(self.history):\n",
        "            entry_length = len(entry['content'])\n",
        "            if total_chars + entry_length <= limit:\n",
        "                truncated.insert(0, entry)\n",
        "                total_chars += entry_length\n",
        "            else:\n",
        "                break\n",
        "        self.history = truncated\n",
        "\n",
        "    def truncate_by_word_limit(self, limit):\n",
        "        total_words = 0\n",
        "        truncated = []\n",
        "        for entry in reversed(self.history):\n",
        "            entry_words = len(entry['content'].split())\n",
        "            if total_words + entry_words <= limit:\n",
        "                truncated.insert(0, entry)\n",
        "                total_words += entry_words\n",
        "            else:\n",
        "                break\n",
        "        self.history = truncated\n",
        "\n",
        "    def get_history(self):\n",
        "        return self.history"
      ],
      "metadata": {
        "id": "RBY0-9fIbF60"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_manager = ConversationManager(summary_interval=3, char_limit=300, word_limit=50)\n",
        "\n",
        "conv_manager.add_message(\"user\", \"Hello!\")\n",
        "conv_manager.add_message(\"assistant\", \"Hi there, how can I help?\")\n",
        "conv_manager.add_message(\"user\", \"Can you explain AI?\")  # Summarization happens here\n",
        "\n",
        "conv_manager.add_message(\"assistant\", \"Sure, AI stands for artificial intelligence.\")\n",
        "conv_manager.add_message(\"user\", \"Give example\")\n",
        "conv_manager.add_message(\"assistant\", \"Like self-driving cars, chatbots, and recommendation systems.\")  # Another summary\n",
        "\n",
        "conv_manager.add_message(\"user\", \"What is ML?\")\n",
        "conv_manager.add_message(\"assistant\", \"Machine learning is part of AI where machines learn from data.\")\n",
        "\n",
        "# Display final history\n",
        "for entry in conv_manager.get_history():\n",
        "    if entry['role'] == \"system\":\n",
        "        print(f\"SYSTEM: Summary of previous conversation: {entry['content']}\")\n",
        "    else:\n",
        "        print(f\"{entry['role'].upper()}: {entry['content']}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7WG-OuvbJnH",
        "outputId": "c2ed5a52-41e6-4d53-e9ae-7b87adc9ed66"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SYSTEM: Summary of previous conversation: USER: Hello! ASSISTANT: Hi there, how can I help? USER: Can you explain AI? ASSISTANT: Sure, AI stands for artificial intelligence. USER: Give example ASSISTANT: Like self-driving cars, chatbots, and recommendation systems.\n",
            "USER: What is ML?\n",
            "ASSISTANT: Machine learning is part of AI where machines learn from data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2HGybe6G0KWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2**"
      ],
      "metadata": {
        "id": "XyPP1F6W0UNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Step 1: Install dependencies\n",
        "# ================================\n",
        "!pip install groq jsonschema\n",
        "\n",
        "# ================================\n",
        "# Step 2: Imports\n",
        "# ================================\n",
        "import os\n",
        "import json\n",
        "from groq import Groq\n",
        "import jsonschema\n",
        "from jsonschema import validate\n",
        "\n",
        "# ================================\n",
        "# Step 3: Set API key\n",
        "# ================================\n",
        "# In Colab, you can store securely:\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")\n",
        "\n",
        "API_KEY = os.environ.get(\"GROQ_API_KEY\")  # make sure key is set in environment\n",
        "client = Groq(api_key=API_KEY)\n",
        "\n",
        "# ================================\n",
        "# Step 4: JSON Schema Definition\n",
        "# ================================\n",
        "schema = {\n",
        "    \"name\": \"extract_user_info\",\n",
        "    \"description\": \"Extract personal details from chat conversations\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"name\": {\"type\": \"string\"},\n",
        "            \"email\": {\"type\": \"string\", \"format\": \"email\"},\n",
        "            \"phone\": {\"type\": \"string\", \"pattern\": \"^[+0-9- ]{7,15}$\"},\n",
        "            \"location\": {\"type\": \"string\"},\n",
        "            \"age\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 120}\n",
        "        },\n",
        "        \"required\": [\"name\", \"email\", \"phone\", \"location\", \"age\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# ================================\n",
        "# Step 5: Extraction Function\n",
        "# ================================\n",
        "def extract_info_from_chat(chat_text):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.3-70b-versatile\",  # ✅ Groq supported model\n",
        "        messages=[{\"role\": \"user\", \"content\": chat_text}],\n",
        "        functions=[schema],\n",
        "        function_call={\"name\": \"extract_user_info\"}\n",
        "    )\n",
        "\n",
        "    # ✅ dot-access instead of dict-style\n",
        "    arguments = response.choices[0].message.function_call.arguments\n",
        "    return json.loads(arguments)\n",
        "\n",
        "# ================================\n",
        "# Step 6: Sample Chats\n",
        "# ================================\n",
        "sample_chats = [\n",
        "    \"Hi, my name is Ramesh Kumar. You can reach me at ramesh.kumar@gmail.com. \"\n",
        "    \"I live in Bangalore. My phone is +91-9876543210 and I am 29 years old.\",\n",
        "\n",
        "    \"This is John. My email is john123@yahoo.com. I’m 35 years old. \"\n",
        "    \"My number is 555-123-4567. I live in New York.\",\n",
        "\n",
        "    \"My name is Priya Sharma, I’m 22. You can call me at +44 7700 900123. \"\n",
        "    \"I stay in London. My email is priya.sharma22@hotmail.com.\"\n",
        "]\n",
        "\n",
        "# ================================\n",
        "# Step 7: Run + Validate\n",
        "# ================================\n",
        "for i, chat in enumerate(sample_chats, 1):\n",
        "    print(f\"\\n📌 Chat {i}: {chat}\\n\")\n",
        "    extracted = extract_info_from_chat(chat)\n",
        "\n",
        "    try:\n",
        "        validate(instance=extracted, schema=schema[\"parameters\"])\n",
        "        print(\"✅ Valid JSON extracted:\")\n",
        "        print(json.dumps(extracted, indent=2))\n",
        "    except jsonschema.exceptions.ValidationError as e:\n",
        "        print(\"❌ Validation error:\", e.message)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95AJxldu3fYM",
        "outputId": "db0c6130-e971-4d8b-9e65-96d959019928"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (0.31.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (4.25.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema) (0.27.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
            "\n",
            "📌 Chat 1: Hi, my name is Ramesh Kumar. You can reach me at ramesh.kumar@gmail.com. I live in Bangalore. My phone is +91-9876543210 and I am 29 years old.\n",
            "\n",
            "✅ Valid JSON extracted:\n",
            "{\n",
            "  \"age\": 29,\n",
            "  \"email\": \"ramesh.kumar@gmail.com\",\n",
            "  \"location\": \"Bangalore\",\n",
            "  \"name\": \"Ramesh Kumar\",\n",
            "  \"phone\": \"+91-9876543210\"\n",
            "}\n",
            "\n",
            "📌 Chat 2: This is John. My email is john123@yahoo.com. I’m 35 years old. My number is 555-123-4567. I live in New York.\n",
            "\n",
            "✅ Valid JSON extracted:\n",
            "{\n",
            "  \"age\": 35,\n",
            "  \"email\": \"john123@yahoo.com\",\n",
            "  \"location\": \"New York\",\n",
            "  \"name\": \"John\",\n",
            "  \"phone\": \"555-123-4567\"\n",
            "}\n",
            "\n",
            "📌 Chat 3: My name is Priya Sharma, I’m 22. You can call me at +44 7700 900123. I stay in London. My email is priya.sharma22@hotmail.com.\n",
            "\n",
            "✅ Valid JSON extracted:\n",
            "{\n",
            "  \"age\": 22,\n",
            "  \"email\": \"priya.sharma22@hotmail.com\",\n",
            "  \"location\": \"London\",\n",
            "  \"name\": \"Priya Sharma\",\n",
            "  \"phone\": \"+44 7700 900123\"\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}